<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
    /* Design Credits: Jon Barron, Deepak Pathak, Saurabh Gupta and Aditya Kusupati*/
    a {
      color: #1772d0;
      text-decoration: none;
    }

    a:focus,
    a:hover {
      color: #f09228;
      text-decoration: none;
    }

    body,
    td,
    th {
      font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
      font-size: 16px;
      font-weight: 400
    }

    heading {
      font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
      font-size: 19px;
      font-weight: 1000
    }

    strong {
      font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
      font-size: 16px;
      font-weight: 800
    }

    strongred {
      font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
      color: 'red';
      font-size: 16px
    }

    sectionheading {
      font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
      font-size: 22px;
      font-weight: 600
    }

    pageheading {
      font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
      font-size: 38px;
      font-weight: 400
    }
  </style>
  <!-- <link rel="icon" type="image/png" href="images/W.png"> -->
  <script type="text/javascript" src="js/hidebib.js"></script>
  <title>Sachin Goyal</title>
  <meta name="Sachin Goyal's Homepage" http-equiv="Content-Type" content="Sachin Goyal's Homepage">
  <link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic'
    rel='stylesheet' type='text/css'>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

  <!-- Scramble Script by Jeff Donahue -->
  <script src="js/scramble.js"></script>
</head>

<body>
  <table width="840" border="0" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr>
      <td>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <p align="center">
            <pageheading>Sachin Goyal</pageheading><br>
            <b>email</b>:
            sachingo@andrew.cmu.edu
            <font id="email" style="display:inline;">
              <noscript><i>Please enable Javascript to view</i></noscript>
            </font>
            <!--      <script>
     emailScramble = new scrambledString(document.getElementById('email'),
          'emailScramble', 'aecsukthoad@wssg.iutn.uinp',
          [14,24,10,15,26,1,7,16,21,6,25,9,13,3,11,19,23,17,4,20,22,12,2,8,18,5]);
     </script> -->
          </p>

          <tr>
            <td width="32%" valign="top"><a href="#Bio"><img src="images/sachin.PNG" width="100%"
                  style="border-radius:15px"></a>
              <p align=center>
                <a href="pubs/CV.pdf" target="_blank">CV</a> | <a
                  href="https://scholar.google.com/citations?user=-KK-60AAAAAJ&hl=en" target="_blank">Scholar</a> | <a
                  href="https://github.com/SachinG007" target="_blank">Github</a> <br>
              </p>
            </td>
            <td width="68%" valign="top" align="justify" id="Bio">
              <p> Hello All !! Welcome to my tiny corner on the web. </p>
              <p> I am a fourth year PhD student in the Machine Learning Department (MLD) at CMU, where I am advised by
                <a href="https://zicokolter.com/" target="_blank"> Prof. Zico Kolter </a>. My current research focus
                includes robust training and finetuning of foundation models.
              </p>
              <p> Prior to CMU, I was a Research Fellow at Microsoft Research, India advised by <a
                  href="https://www.prateekjain.org/" target="_blank"> Prateek Jain </a> and <a
                  href="http://harsha-simhadri.org/" target="_blank"> Harsha Vardhan Simhadri </a>. I worked on <a
                  href="https://github.com/microsoft/EdgeML" target="_blank">EdgeML</a>, developing ML algorithms for
                severely resource constrained devices. </p>
              <p> Earlier, I spent 4 amazing years at IIT Bombay, earning a Bachelor's in EE (CGPA 9.11) with a Minor in
                CS. I was advised by <a href="https://www.ee.iitb.ac.in/~sc/" target="_blank"> Subhasis Chaudhuri </a>
                for my bachleor's thesis.</p>

            </td>
          </tr>
        </table>


        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
          <tr>
            <td id="Preprints">
              <sectionheading>&nbsp;&nbsp;Preprints</sectionheading>
            </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">


          <tr>
            <td width="33%" valign="top" align="center"><a href="https://context-parametric-inversion.github.io/"
                target="_blank"><img src="images/cpi.png" alt="CPI" width="100%" style="border-radius:15px"></a>
            <td width="67%" valign="top">
              <p><a href="https://context-parametric-inversion.github.io/" target="_blank" id="cpi">
                  <heading>Context-Parametric Inversion: Why Instruction Finetuning May Not Actually Improve Context
                    Reliance
                  </heading>
                </a><br>
                <strong>Sachin Goyal</strong><sup>*</sup>,
                <a href="https://kebaek.github.io/" target="_blank"> Christina Baek</a><sup>*</sup>,
                <a href="https://zicokolter.com/" target="_blank"> Zico Kolter</a>,
                <a href="https://www.cs.cmu.edu/~aditirag/" target="_blank"> Aditi Raghunathan </a> <br>
                Under Review<br>
              </p>

              <div class="paper" id="fadu">
                <a href="https://context-parametric-inversion.github.io/">abstract</a> /
                <a href="https://arxiv.org/abs/2410.10796" target="_blank">paper</a> /
                <a href="https://context-parametric-inversion.github.io/" target="_blank">website</a>
                <br>

                <p align="justify"> <i id="cpiabs">

                  </i>
                </p>

          </tr>




        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
          <tr>
            <td id="ConfPublications">
              <sectionheading>&nbsp;&nbsp;Publications</sectionheading>
            </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">



          <tr>
            <td width="33%" valign="top" align="center"><a href="https://arxiv.org/abs/2404.07177" target="_blank"><img
                  src="images/fadu.png" alt="FADU" width="100%" style="border-radius:15px"></a>
            <td width="67%" valign="top">
              <p><a href="https://arxiv.org/abs/2404.07177" target="_blank" id="fadu">
                  <heading>Scaling Laws for Data Filtering--Data Curation cannot be Compute Agnostic
                  </heading>
                </a><br>
                <strong>Sachin Goyal</strong><sup>*</sup>,
                <a href="https://pratyushmaini.github.io/" target="_blank"> Pratyush Maini</a><sup>*</sup>,
                <a href="https://acmilab.org/" target="_blank"> Zachary
                  Lipton</a>, <a href="https://www.cs.cmu.edu/~aditirag/" target="_blank"> Aditi Raghunathan </a>,
                <a href="https://zicokolter.com/" target="_blank"> Zico Kolter</a> <br>
                Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2024<br>
              </p>

              <div class="paper" id="fadu">
                <a href="javascript:toggleblock('pausellmabs')">abstract</a> /
                <a href="https://arxiv.org/abs/2404.07177" target="_blank">paper</a>
                <br>

                <p align="justify"> <i id="faduabs">

                  </i>
                </p>

          </tr>

          <tr>
            <td width="33%" valign="top" align="center"><a href="https://arxiv.org/abs/2310.02226" target="_blank"><img
                  src="images/pauseft.png" alt="PauseLLM" width="100%" style="border-radius:15px"></a>
            <td width="67%" valign="top">
              <p><a href="https://arxiv.org/abs/2310.02226" target="_blank" id="pausellm">
                  <heading>Think before you speak: Training Language Models With Pause Tokens
                  </heading>
                </a><br>
                <strong>Sachin Goyal</strong>, <a href="https://jiziwei.github.io/" target="_blank"> Ziwei
                  Ji</a>, <a href="https://ankitsrawat.github.io/home/" target="_blank"> Ankit Singh Rawat</a>, <a
                  href="https://akmenon.github.io/" target="_blank"> Aditya Krishna Menon</a>, <br><a
                  href="https://scholar.google.com/citations?user=08CNqrYAAAAJ&hl=en" target="_blank"> Sanjiv Kumar</a>,
                <a href="https://vaishnavh.github.io/home/index.html" target="_blank"> Vaishnavh Nagarajan </a> <br>
                International Conference on Learning Representations (<strong>ICLR</strong>) 2024<br>
              </p>

              <div class="paper" id="pausellm">
                <a href="javascript:toggleblock('pausellmabs')">abstract</a> /
                <a href="https://arxiv.org/abs/2310.02226" target="_blank">paper</a>
                <br>

                <p align="justify"> <i id="pausellmabs">Language models generate responses by producing a series of
                    tokens in immediate
                    succession: the (K +1)th token is an outcome of manipulating K hidden vectors
                    per layer, one vector per preceding token. What if instead we were to let the model
                    manipulate say, K+10 hidden vectors, before it outputs the (K+1)th token? We
                    operationalize this idea by performing training and inference on language models
                    with a (learnable) pause token, a sequence of which is appended to the input
                    prefix. We then delay extracting the model's outputs until the last pause token is
                    seen, thereby allowing the model to process extra computation before committing
                    to an answer. We empirically evaluate pause-training on decoder-only models
                    of 1B and 130M parameters with causal pretraining on C4, and on downstream
                    tasks covering reasoning, question-answering, general understanding and fact recall.
                    Our main finding is that inference-time delays show gains on our tasks when
                    the model is both pre-trained and finetuned with delays. For the 1B model, we
                    witness gains on eight tasks, most prominently, a gain of 18% EM score on the
                    QA task of SQuAD, 8% on CommonSenseQA and 1% accuracy on the reasoning
                    task of GSM8k. Our work raises a range of conceptual and practical future
                    research questions on making delayed next-token prediction a widely applicable
                    new paradigm.</i>
                </p>

          </tr>


          <tr>
            <td width="33%" valign="top" align="center"><a href="https://arxiv.org/abs/2307.03132" target="_blank"><img
                  src="images/tmars.png" alt="TMARS" width="100%" style="border-radius:15px"></a>
            <td width="67%" valign="top">
              <p><a href="https://arxiv.org/abs/2307.03132" target="_blank" id="tmars">
                  <heading>T-MARS: Improving Visual Representations by Circumventing Text Feature Learning
                  </heading>
                </a><br>
                <a href="https://pratyushmaini.github.io/" target="_blank"> Pratyush Maini</a><sup>*</sup>,
                <strong>Sachin Goyal</strong><sup>*</sup>, <a href="https://acmilab.org/" target="_blank"> Zachary
                  Lipton</a>, <a href="https://zicokolter.com/" target="_blank"> Zico Kolter</a>,<br> <a
                  href="https://www.cs.cmu.edu/~aditirag/" target="_blank"> Aditi
                  Raghunathan </a> <br>
                Datacomp Workshop @ <strong>ICCV</strong> 2023 <span
                  style="color:darkgreen;"><strong>(oral)</strong></span><br>
                International Conference on Learning Representations (<strong>ICLR</strong>) 2024<br>

              </p>

              <div class="paper" id="tmars">
                <a href="javascript:toggleblock('tmarsabs')">abstract</a> /
                <a href="https://arxiv.org/abs/2307.03132" target="_blank">paper</a> /
                <a href="https://tmars-clip.github.io/" target="_blank">project page</a>
                <br>

                <p align="justify"> <i id="tmarsabs">Large web-sourced multimodal datasets have powered a slew of new
                    methods for learning general-purpose visual representations, advancing the state of the art in
                    computer vision and revolutionizing zero- and few-shot recognition. One crucial decision facing
                    practitioners is how, if at all, to curate these ever-larger datasets. For example, the creators of
                    the LAION-5B dataset chose to retain only image-caption pairs whose CLIP similarity score exceeded a
                    designated threshold. In this paper, we propose a new state-of-the-art data filtering approach
                    motivated by our observation that nearly 40% of LAION's images contain text that overlaps
                    significantly with the caption. Intuitively, such data could be wasteful as it incentivizes models
                    to perform optical character recognition rather than learning visual features. However, naively
                    removing all such data could also be wasteful, as it throws away images that contain visual features
                    (in addition to overlapping text). Our simple and scalable approach, T-MARS (Text Masking and
                    Re-Scoring), filters out only those pairs where the text dominates the remaining visual features --
                    by first masking out the text and then filtering out those with a low CLIP similarity score of the
                    masked image. Experimentally, T-MARS outperforms the top-ranked method on the "medium scale" of
                    DataComp (a data filtering benchmark) by a margin of 6.5% on ImageNet and 4.7% on VTAB.
                    Additionally, our systematic evaluation on various data pool sizes from 2M to 64M shows that the
                    accuracy gains enjoyed by T-MARS linearly increase as data and compute are scaled exponentially.</i>
                </p>

          </tr>



          <tr>
            <td width="33%" valign="top" align="center"><a href="https://arxiv.org/abs/2212.00638" target="_blank"><img
                  src="images/flyp.png" alt="FLYP" width="100%" style="border-radius:15px"></a>
            <td width="67%" valign="top">
              <p><a href="https://arxiv.org/abs/2212.00638" target="_blank" id="met">
                  <heading>Finetune like you pretrain: Improved finetuning of zero-shot vision models</heading>
                </a><br>
                <strong>Sachin Goyal</strong>, <a href="https://ananyakumar.wordpress.com/" target="_blank"> Ananya
                  Kumar</a>, Sankalp Garg, <a href="https://zicokolter.com/" target="_blank"> Zico Kolter</a>, <a
                  href="https://www.cs.cmu.edu/~aditirag/" target="_blank"> Aditi
                  Raghunathan </a> <br>
                Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2023.<br>
              </p>

              <div class="paper" id="flyp">
                <a href="javascript:toggleblock('flypabs')">abstract</a> /
                <a href="https://arxiv.org/abs/2212.00638" target="_blank">paper</a> /
                <a href="https://nips.cc/virtual/2022/workshop/50018#collapse56262:~:text=Zico%20Kolter%2C%20Adapt%20like%20you%20train%3A%20How%20optimization%20at%20training%20time%20affects%20model%20finetuning%20and%20adaptation"
                  target="_blank">talk</a>

                <br>

                <p align="justify"> <i id="flypabs">Finetuning image-text models such as CLIP achieves state-of-the-art
                    accuracies on a variety of benchmarks. However, recent works like WiseFT (Wortsman et al., 2021) and
                    LP-FT (Kumar et al., 2022) have shown that even subtle differences in the finetuning process can
                    lead to surprisingly large differences in the final performance, both for in-distribution (ID) and
                    out-of-distribution (OOD) data. In this work, we show that a natural and simple approach of
                    mimicking contrastive pretraining consistently outperforms alternative finetuning approaches.
                    Specifically, we cast downstream class labels as text prompts and continue optimizing the
                    contrastive loss between image embeddings and class-descriptive prompt embeddings (contrastive
                    finetuning).

                    Our method consistently outperforms baselines across 7 distribution shifts, 6 transfer learning, and
                    3 few-shot learning benchmarks. On WILDS-iWILDCam, our proposed approach FLYP outperforms the top of
                    the leaderboard by 2.3% ID and 2.7% OOD, giving the highest reported accuracy. Averaged across 7 OOD
                    datasets (2 WILDS and 5 ImageNet associated shifts), FLYP gives gains of 4.2% OOD over standard
                    finetuning and outperforms the current state of the art (LP-FT) by more than 1% both ID and OOD.
                    Similarly, on 3 few-shot learning benchmarks, our approach gives gains up to 4.6% over standard
                    finetuning and 4.4% over the state of the art. In total, these benchmarks establish contrastive
                    finetuning as a simple, intuitive, and state-of-the-art approach for supervised finetuning of
                    image-text models like CLIP.</i></p>

          </tr>

          <tr>
            <td width="33%" valign="top" align="center"><a href="https://arxiv.org/pdf/2207.09640.pdf"
                target="_blank"><img src="images/conj.png" alt="conj" width="100%" style="border-radius:15px"></a>
            <td width="67%" valign="top">
              <p><a href="https://arxiv.org/pdf/2207.09640.pdf" target="_blank" id="conj">
                  <heading>Test-Time Adaptation via Conjugate Pseudo-labels</heading>
                </a><br>
                <strong>Sachin Goyal<sup>*</sup></strong>, Mingjie Sun<sup>*</sup>, <a
                  href="https://www.cs.cmu.edu/~aditirag/" target="_blank"> Aditi Raghunathan</a>, <a
                  href="https://zicokolter.com/" target="_blank"> Zico Kolter </a> <br>
                Neural Information Processing Systems (<strong>NeurIPS</strong>), 2022.<br>
              </p>

              <div class="paper" id="conj">
                <a href="javascript:toggleblock('conjabs')">abstract</a> /
                <a href="https://arxiv.org/pdf/2207.09640.pdf" target="_blank">paper</a> /
                <a href="https://youtu.be/K_gnOeQjjGM?t=1704" target="_blank">talk</a>
                <br>

                <p align="justify"> <i id="conjabs">Test-time adaptation (TTA) refers to adapting neural networks to
                    distribution shifts, with access to only the unlabeled test samples from the new domain at
                    test-time. Prior TTA methods optimize over unsupervised objectives such as the entropy of model
                    predictions in TENT [Wang et al., 2021], but it is unclear what exactly makes a good TTA loss. In
                    this paper, we start by presenting a surprising phenomenon: if we attempt to meta-learn the best
                    possible TTA loss over a wide class of functions, then we recover a function that is remarkably
                    similar to (a temperature-scaled version of) the softmax-entropy employed by TENT. This only holds,
                    however, if the classifier we are adapting is trained via cross-entropy; if trained via squared
                    loss, a different best TTA loss emerges. To explain this phenomenon, we analyze TTA through the lens
                    of the training losses's convex conjugate. We show that under natural conditions, this
                    (unsupervised) conjugate function can be viewed as a good local approximation to the original
                    supervised loss and indeed, it recovers the best losses found by meta-learning. This leads to a
                    generic recipe that can be used to find a good TTA loss for any given supervised training loss
                    function of a general class. Empirically, our approach consistently dominates other baselines over a
                    wide range of benchmarks. Our approach is particularly of interest when applied to classifiers
                    trained with novel loss functions, e.g., the recently-proposed PolyLoss, where it differs
                    substantially from (and outperforms) an entropy-based loss. Further, we show that our approach can
                    also be interpreted as a kind of self-training using a very specific soft label, which we refer to
                    as the conjugate pseudolabel. Overall, our method provides a broad framework for better
                    understanding and improving test-time adaptation. Code is available at
                    https://github.com/locuslab/tta_conjugate.</i></p>

          </tr>


          <tr>
            <td width="33%" valign="top" align="center"><a href="https://arxiv.org/pdf/2206.08564.pdf"
                target="_blank"><img src="images/met.png" alt="MET" width="100%" style="border-radius:15px"></a>
            <td width="67%" valign="top">
              <p><a href="https://arxiv.org/pdf/2206.08564.pdf" target="_blank" id="met">
                  <heading>MET : Masked Encoding for Tabular Data</heading>
                </a><br>
                Kushal Majmundar, <strong>Sachin Goyal</strong>, <a href="https://praneethnetrapalli.org/"
                  target="_blank"> Praneeth Netrapalli</a>, <a href="https://www.prateekjain.org/" target="_blank">
                  Prateek Jain </a> <br>
                Table Representation Learning Workshop @ <strong>NeurIPS</strong> 2022<br>
              </p>

              <div class="paper" id="met">
                <a href="javascript:toggleblock('metabs')">abstract</a> /
                <a href="https://arxiv.org/pdf/2206.08564.pdf" target="_blank">paper</a>
                <br>

                <p align="justify"> <i id="metabs">We consider the task of self-supervised representation learning (SSL)
                    for tabular data: tabular-SSL. Typical contrastive learning based SSL methods require instance-wise
                    data augmentations which are difficult to design for unstructured tabular data. Existing tabular-SSL
                    methods design such augmentations in a relatively ad-hoc fashion and can fail to capture the
                    underlying data manifold. Instead of augmentations based approaches for tabular-SSL, we propose a
                    new reconstruction based method, called Masked Encoding for Tabular Data (MET), that does not
                    require augmentations. MET is based on the popular MAE approach for vision-SSL [He et al., 2021] and
                    uses two key ideas: (i) since each coordinate in a tabular dataset has a distinct meaning, we need
                    to use separate representations for all coordinates, and (ii) using an adversarial reconstruction
                    loss in addition to the standard one. Empirical results on five diverse tabular datasets show that
                    MET achieves a new state of the art (SOTA) on all of these datasets and improves up to 9% over
                    current SOTA methods. We shed more light on the working of MET via experiments on carefully designed
                    simple datasets.</i></p>

          </tr>


          <tr>
            <td width="33%" valign="top" align="center"><a href="pubs/pal.pdf" target="_blank"><img src="images/pal.PNG"
                  alt="PAL" width="100%" style="border-radius:15px"></a>
            <td width="67%" valign="top">
              <p><a href="pubs/pal.pdf" target="_blank" id="pal">
                  <heading>PAL: Pretext-based Active Learning</heading>
                </a><br>
                Shubhang Bhatnagar, <strong>Sachin Goyal</strong>, Darshan Tank, <a
                  href="https://www.ee.iitb.ac.in/~asethi/" target="_blank"> Amit Sethi </a> <br>
                British Machine Vision Conference (<strong>BMVC</strong>), 2021<br>
              </p>

              <div class="paper" id="pal">
                <a href="javascript:toggleblock('palabs')">abstract</a> /
                <a href="https://arxiv.org/pdf/2010.15947.pdf" target="_blank">paper</a>
                <br>

                <p align="justify"> <i id="palabs">The goal of pool-based active learning is to judiciously select a
                    fixed-sized subset of unlabeled samples from a pool to query an oracle for their labels, in order to
                    maximize the accuracy of a supervised learner. However, the unsaid requirement that the oracle
                    should always assign correct labels is unreasonable for most situations. We propose an active
                    learning technique for deep neural networks that is more robust to mislabeling than the previously
                    proposed techniques. Previous techniques rely on the task network itself to estimate the novelty of
                    the unlabeled samples, but learning the task (generalization) and selecting samples
                    (out-of-distribution detection) can be conflicting goals. We use a separate network to score the
                    unlabeled samples for selection. The scoring network relies on self-supervision for modeling the
                    distribution of the labeled samples to reduce the dependency on potentially noisy labels. To counter
                    the paucity of data, we also deploy another head on the scoring network for regularization via
                    multi-task learning and use an unusual self-balancing hybrid scoring function. Furthermore, we
                    divide each query into sub-queries before labeling to ensure that the query has diverse samples. In
                    addition to having a higher tolerance to mislabeling of samples by the oracle, the resultant
                    technique also produces competitive accuracy in the absence of label noise. The technique also
                    handles the introduction of new classes on-the-fly well by temporarily increasing the sampling rate
                    of these classes.</i></p>

          </tr>

          <tr>
            <td width="33%" valign="top" align="center"><a href="pubs/drocc.pdf" target="_blank"><img
                  src="images/drocc.PNG" alt="DROCC" width="100%" style="border-radius:15px"></a>
            <td width="67%" valign="top">
              <p><a href="pubs/drocc.pdf" target="_blank" id="drocc">
                  <heading>DROCC: Deep Robust One-Class Classification</heading>
                </a><br>
                <strong>Sachin Goyal</strong>, <a href="https://stanford.edu/~aditir/" target="_blank"> Aditi
                  Raghunathan</a>, Moksh Jain, <a href="http://harsha-simhadri.org/" target="_blank"> Harsha Vardhan
                  Simhadri </a>, <a href="https://www.prateekjain.org/" target="_blank"> Prateek Jain </a> <br>
                International Conference on Machine Learning (<strong>ICML</strong>), 2020<br>
              </p>

              <div class="paper" id="drocc">
                <a href="javascript:toggleblock('droccabs')">abstract</a> /
                <a href="https://proceedings.icml.cc/book/4293.pdf" target="_blank">paper</a> /
                <a href="https://github.com/microsoft/EdgeML" target="_blank">code</a> /
                <a href="https://www.youtube.com/watch?v=20pDzeSgSfw&t=789s" target="_blank">video</a>
                <br>

                <p align="justify"> <i id="droccabs">Classical approaches for one-class problems such as one-class SVM
                    and isolation forest require careful feature engineering when applied to structured domains like
                    images. State-of-the-art methods aim to leverage deep learning to learn appropriate features via two
                    main approaches. The first approach based on predicting transformations (Golan & El-Yaniv, 2018;
                    Hendrycks et al., 2019a) while successful in some domains, crucially depends on an appropriate
                    domain-specific set of transformations that are hard to obtain in general. The second approach of
                    minimizing a classical one-class loss on the learned final layer representations, e.g., DeepSVDD
                    (Ruff et al., 2018) suffers from the fundamental drawback of representation collapse. In this work,
                    we propose Deep Robust One Class Classification (DROCC) that is both applicable to most standard
                    domains without requiring any side-information and robust to representation collapse. DROCC is based
                    on the assumption that the points from the class of interest lie on a well-sampled, locally linear
                    low dimensional manifold. Empirical evaluation demonstrates that DROCC is highly effective in two
                    different one-class problem settings and on a range of real-world datasets across different domains:
                    tabular data, images (CIFAR and ImageNet), audio, and time-series, offering up to 20% increase in
                    accuracy over the state-of-the-art in anomaly detection. DROCC's code is available at <a
                      href="https://github.com/Microsoft/EdgeML/">https://github.com/Microsoft/EdgeML/</a>.</i></p>

          </tr>


          <tr>
            <td width="33%" valign="top" align="center"><a href="pubs/lps.pdf" target="_blank"><img src="images/lps.PNG"
                  alt="LPS" width="100%" style="border-radius:15px"></a>
            <td width="67%" valign="top">
              <p><a href="pubs/lps.pdf" target="_blank" id="lps">
                  <heading>Indoor Distance Estimation using LSTMs over WLAN Network</heading>
                </a><br>
                <a href="https://sabsathai.github.io/" target="_blank"> Pranav Sankhe</a>, Saqib Azim, <strong> Sachin
                  Goyal </strong>, Tanya Choudhary,<a href="https://www.ee.iitb.ac.in/~akumar/" target="_blank"> Kumar
                  Appaiah </a>, <a href="https://www.sc.iitb.ac.in/~srikant/dokuwiki/doku.php" target="_blank"> Sukumar
                  Srikant </a> <br>
                India Patent Application 201821047043, filed Dec' 2018. Patent Pending.<br>
                Workshop on Positioning, Navigation and Communications (WPNC), 2019<br>
              </p>

              <div class="paper" id="lps">
                <a href="javascript:toggleblock('lpsabs')">abstract</a> /
                <a href="https://ieeexplore.ieee.org/abstract/document/8970257" target="_blank">paper</a> /
                <a href="https://arxiv.org/pdf/2003.13991.pdf" target="_blank">arxiv</a> /
                <a href="pubs/LPS_ppt.pdf" target="_blank">presentation</a>
                <br>

                <p align="justify"> <i id="lpsabs">The Global Navigation Satellite Systems (GNSS)like GPS suffer from
                    accuracy degradation and are almostunavailable in indoor environments. Indoor positioning
                    systems(IPS) based on WiFi signals have been gaining popularity.However, owing to the strong spatial
                    and temporal variationsof wireless communication channels in the indoor environment,the achieved
                    accuracy of existing IPS is around several tens ofcentimeters. We present the detailed design and
                    implementationof a self-adaptive WiFi-based indoor distance estimation systemusing LSTMs. The system
                    is novel in its method of estimatingwith high accuracy the distance of an object by
                    overcomingpossible causes of channel variations and is self-adaptive tothe changing environmental
                    and surrounding conditions. Theproposed design has been developed and physically realized overa WiFi
                    network consisting of ESP8266 (NodeMCU) devices. Theexperiments were conducted in a real indoor
                    environment whilechanging the surroundings in order to establish the adaptabilityof the system. We
                    compare different architectures for this taskbased on LSTMs, CNNs, and fully connected networks
                    (FCNs).We show that the LSTM based model performs better amongall the above-mentioned architectures
                    by achieving an accuracyof5.85cm with a confidence interval of93%on the scale of(8.46m x6.98m). To
                    the best of our knowledge, the proposedmethod outperforms other methods reported in the literature
                    bya significant margin.</p>

          </tr>

          <tr>
            <td width="33%" valign="top" align="center"><a href="pubs/spie.pdf" target="_blank"><img
                  src="images/spie.PNG" alt="SPIE" width="100%" style="border-radius:15px"></a>
            <td width="67%" valign="top">
              <p><a href="pubs/spie.pdf" target="_blank" id="spie">
                  <heading>Improving self super resolution in magnetic resonance images</heading>
                </a><br>
                <strong> Sachin Goyal </strong>, Can Zhao,<a href="https://asjog.github.io/" target="_blank"> Amod Jog
                </a>, Aaron Carass, <a href="https://engineering.jhu.edu/ece/faculty/prince-jerry-l/" target="_blank">
                  Jerry L. Prince </a> <br>
                SPIE Conference on Medical Imaging and Biomedical Applications, 2018 <br>
              </p>

              <div class="paper" id="spie">
                <a href="javascript:toggleblock('spieabs')">abstract</a> /
                <a href="https://ieeexplore.ieee.org/abstract/document/8970257" target="_blank">paper</a> /
                <a href="https://arxiv.org/pdf/2003.13991.pdf" target="_blank">arxiv</a>
                <br>

                <p align="justify"> <i id="spieabs">Magnetic resonance (MR) images (MRI) are routinely acquired with
                    high in-plane resolution and lower through-plane resolution. Improving the resolution of such data
                    can be achieved through post-processing techniques knows as super-resolution (SR), with various
                    frameworks in existence. Many of these approaches rely on external databases from which SR methods
                    infer relationships between low and high resolution data. The concept of self super-resolution (SSR)
                    has been previously reported, wherein there is no external training data with the method only
                    relying on the acquired image. The approach involves extracting image patches from the acquired
                    image constructing new images based on regression and combining the new images by Fourier Burst
                    Accumulation. In this work, we present four improvements to our previously reported SSR approach. We
                    demonstrate these improvements have a significant effect on improving image quality and the measured
                    resolution.</p>

          </tr>

        </table>


        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
          <br />
          <tr>
            <td id="Software">
              <sectionheading>&nbsp;&nbsp;Software</sectionheading>
            </td>
          </tr>
        </table>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="33%" valign="top"><a href="https://github.com/microsoft/EdgeML" target="_blank"><img
                  src="images/edgeml.png" alt="EdgeML" width="100%" style="border-radius:15px"></a>
            <td width="67%" valign="top">
              <p><a href="https://github.com/microsoft/EdgeML" id="EdgeML" target="_blank">
                  <heading>EdgeML: Machine Learning for resource-constrained edge devices</heading>
                </a><br>
                Work of many amazing collaborators. I am one of the current collaborator.<br>
                <!-- <a href="https://dkdennis.xyz/">Don Dennis</a>, <a href="http://www.sridhargopinath.in">Sridhar Gopinath</a>, <a href="https://aigen.github.io/">Chirag Gupta</a>, <a href="https://ashishkumar1993.github.io/">Ashish Kumar</a>, <strong>Aditya Kusupati</strong>, <a href="https://shishirpatil.github.io/">Shishir Patil</a> and <a href="http://harsha-simhadri.org/">Harsha Vardhan Simhadri</a> (&alpha;&beta; ordering)<br> -->
                Github, Microsoft Research India, 2017-present.
                <br>
              </p>

              <div class="paper" id="edgeml">
                <a href="javascript:toggleblock('edgemlabs')">abstract</a> /
                <a shape="rect" href="javascript:togglebib('edgeml')" class="togglebib">bibtex</a>

                <p align="justify"> <i id="edgemlabs">Open source repository for all the research outputs on resource
                    efficient Machine Learning from Microsoft Research India. It contains scalable and multi-framework
                    compatible implementations of Bonsai, ProtoNN, FastCells, EMI-RNN, ShaRNN, RNNPool, DROCC, a tool
                    named SeeDot for fixed-point compilation of ML models along with applications such as on-device
                    Keyword spotting and Gesturepod.</i><br>EdgeML is under MIT license and is open to contributions and
                  suggestions. Please <a shape="rect" href="javascript:togglebib('edgeml')" class="togglebib">cite</a>
                  the software if you happen to use EdgeML in your research or otherwise (use the latest bibtex from the
                  repository in case this gets outdated)</p>

                <pre xml:space="preserve">
@misc{edgeml03,
    author = {{Dennis, Don Kurian and Gaurkar, Yash and 
      Gopinath, Sridhar and Gupta, Chirag and
      Jain, Moksh and Kumar, Ashish and
      Kusupati, Aditya and Lovett, Chris and
      Patil, Shishir Girish and Simhadri, Harsha Vardhan}},
    title = {{EdgeML: Machine Learning 
      for resource-constrained edge devices}},
    url = {https://github.com/Microsoft/EdgeML},
    version = {0.3},
}</pre>
              </div>
            </td>
          </tr>
        </table>


        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
          <tr>
            <td id="Miscellaneous">
              <sectionheading>&nbsp;&nbsp;Miscellaneous</sectionheading>
            </td>
          </tr>
        </table>


        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="33%" valign="top" align="center"><a href="dpac.html" target="_blank"><img src="images/dpac.PNG"
                  alt="DPAC" width="100%" style="border-radius:15px"></a>
            <td width="67%" valign="top">
              <p><a href="dpac.html" target="_blank" id="dpac">
                  <heading>DPAC: Digitally Programmable Analog Computer</heading>
                </a><br>
                <a href="https://people.eecs.berkeley.edu/~shah/" target="_blank"> Dhruv Shah</a>, <strong>Sachin
                  Goyal</strong>, Srivatsan Sridhar <br>
              </p>

              <div class="paper" id="dpac">
                <a href="dpac.html">abstract</a> /
                <a href="pubs/dpac.pdf" target="_blank">Technical Report</a>
                <br>

                <p align="justify"> <i id="dpacabs"> Hardware-in-the-loop simulations are very commonly used to test
                    controller design and monitor how the controller responds, in real time, to realistic virtual
                    stimuli. In an HIL simulation, a real-time computer is used as a virtual representation of the plant
                    model and a real version of the concerned controller. Most of these dynamical systems are in the
                    form of coupled differential equations, and digital computers tend to be terribly slow at
                    iteratively approximating solutions to such systems. The notion of using analog computing grids to
                    efficiently solve differential equations (in hardware) has been well accepted in the research
                    fraternity, and proves to be a faster way to solve linear dynamical systems.
                    In this project, we demonstrate a digitally programmable analog computer, which can solve linear
                    dynamical systems with upto 5 state variables. The system is capable of working in real time, since
                    there are no moving parts once the configuration is set and the system is programmed. The system is
                    capable of being driven by upto 5 forcing functions, and can represent any linear dynamical system
                    upto order 5.
                    It consists of active devices to implement integrators, gain blocks and inverter blocks using
                    operational amplifiers, along with passive components to emulate the system matrix. These blocks
                    will be linked together using analog switches which would be controlled by signals given by a
                    microcontroller. For our first prototype, we assume B and C to be identity, for the sake of
                    simplicity.
                    In this report, we present the design philosophies, layout descriptions, experimental results and
                    analyses of two prototypes ㅡ DPAC-𝜷 and DPACv1.0. The DPAC-𝜷 is a miniature version of the
                    DPACv1.0, to emulate second order systems, and features a block-modular structure and mechanical
                    switches, allowing easy configuration of the system matrix and operational parameters. The DPACv1.0
                    features a single PCB, is interfaced and controlled using a microcontroller, and is capable of
                    solving the linear dynamical system in real time.</p>

          </tr>


          <tr>
            <td width="33%" valign="top" align="center"><img src="images/graphics.PNG" alt="DPAC" width="90%"
                style="border-radius:15px">
            <td width="67%" valign="top">
              <p>
                <heading>The Music Box Short Film</heading><br>
                <strong>Sachin Goyal</strong>, Arpan Banerjee <br>
              </p>

              <div class="paper" id="graphics">
                <a href="javascript:toggleblock('graphicsabs')">abstract</a> /
                <a href="https://www.youtube.com/watch?v=DysZUCfkbf0" target="_blank">Video</a>
                <br>

                <p align="justify"> <i id="graphicsabs"> Created an animated film with a music box and two humanoids
                    using hierarchical modelling in OpenGL+. Wrote GLSL shaders to implement Gouraud shading for
                    humanoids and apply textures to room. </p>

          </tr>

        </table>


        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
          <tr>
            <td id="Teaching">
              <sectionheading>&nbsp;&nbsp;Teaching</sectionheading>
            </td>
          </tr>
        </table>
        <table width="100%" align="center" border="0" cellpadding="20">
          <tr>
            <td width="33%" valign="top"><a href="#Teaching"><img src="images/teaching.png" alt="teaching" width="90%"
                  style="border-radius:15px"></a>
            <td width="67%" valign="top">
              <p>
                <b>BB101: Biology, Fall '17, IIT Bombay</b><br>
                <!-- <strong>Instructor</strong>: Prof. Supratik Chakraborty<br> -->
              </p>
              <p>
                <b>Teaching basics of Programing to High School Students in hometown, Pandemic '2020, Udaipur </b><br>
                <!-- <strong>Instructor</strong>: Prof. Sharat Chandran<br> -->
              </p>
            </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellpadding="10">
          <tr>
            <td id="Misc">
              <sectionheading>&nbsp;&nbsp;Misc</sectionheading>
              <ul>
                <li> A short blog on cracking japanese placement interviews <a href="pubs/sony.pdf"
                    target="_blank">here</a> </li>
              </ul>
            </td>
          </tr>
        </table>

        <a href="http://s01.flagcounter.com/more/oc"><img
            src="https://s01.flagcounter.com/count2/oc/bg_FFFFFF/txt_000000/border_CCCCCC/columns_2/maxflags_10/viewers_0/labels_1/pageviews_1/flags_0/percent_0/"
            alt="Flag Counter" border="0"></a>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td><br>
              <p align="right">
                <font size="2">
                  Template: <a href="https://jonbarron.info">this</a>, <a
                    href="https://people.eecs.berkeley.edu/~pathak/">this</a>, <a
                    href="http://saurabhg.web.illinois.edu/">this</a> and <a
                    href="https://homes.cs.washington.edu/~kusupati/">this</a>
                </font>
              </p>
            </td>
          </tr>
        </table>


      </td>
    </tr>
  </table>
  <script xml:space="preserve" language="JavaScript">
    hideallbibs();
  </script>
  <script xml:space="preserve" language="JavaScript">
    hideblock('spieabs');
  </script>
  <script xml:space="preserve" language="JavaScript">
    hideblock('droccabs');
  </script>
  <script xml:space="preserve" language="JavaScript">
    hideblock('palabs');
  </script>
  <script xml:space="preserve" language="JavaScript">
    hideblock('pausellmabs');
  </script>
  <script xml:space="preserve" language="JavaScript">
    hideblock('metabs');
  </script>
  <script xml:space="preserve" language="JavaScript">
    hideblock('icmlabs');
  </script>
  <script xml:space="preserve" language="JavaScript">
    hideblock('flypabs');
  </script>
  <script xml:space="preserve" language="JavaScript">
    hideblock('tmarsabs');
  </script>
  <script xml:space="preserve" language="JavaScript">
    hideblock('conjabs');
  </script>
  <script xml:space="preserve" language="JavaScript">
    hideblock('lpsabs');
  </script>
  <script xml:space="preserve" language="JavaScript">
    hideblock('dpacabs');
  </script>
  <script xml:space="preserve" language="JavaScript">
    hideblock('graphicsabs');
  </script>
  <script xml:space="preserve" language="JavaScript">
    hideblock('edgemlabs');
  </script>

</body>

</html>